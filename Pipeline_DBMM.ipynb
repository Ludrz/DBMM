{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442e5efb",
   "metadata": {},
   "source": [
    "# Pipeline Completo de ETL para o Banco de Dados `test_4_DBMM`\n",
    "\n",
    "**Autor:** Luiz Felipe Monteiro Darzé\n",
    "\n",
    "\n",
    "## Descrição\n",
    "Este notebook implementa o pipeline completo de Extração, Transformação e importação para o projeto de mieloma múltiplo. O fluxo de trabalho é dividido em três etapas sequenciais e verificáveis:\n",
    "\n",
    "1.  **Organização dos Arquivos:** Conta os arquivos nas fontes de dados (`Google Drive`, `Mendeley`), **move-os** para uma estrutura de diretório padronizada em `REPOSITÓRIO_DBMM` e verifica se todos os arquivos foram movidos corretamente.\n",
    "2.  **Extração para CSV:** Lê a estrutura organizada, extrai metadados relevantes e gera 9 arquivos `.csv`, um para cada tabela do banco de dados, na pasta `Output_CSV`.\n",
    "3.  **Importação do Banco de Dados:** Carrega os dados dos arquivos `.csv` para o banco de dados MySQL `test_4_DBMM` e valida a contagem de registros para confirmar a integridade da importação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb009cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAÇÕES E CONFIGURAÇÃO\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "\n",
    "# Configuração do Logging \n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#  CONFIGURAÇÃO DE CAMINHOS \n",
    "GDRIVE_SOURCE_PATH = Path(\"/home/ludrz/Documentos/Trabalhos/IC/Data projeto Mieloma Caio/Dados test_4_DBMM/DADOS PCMMD - Google drive\")\n",
    "MENDELEY_SOURCE_PATH = Path(\"/home/ludrz/Documentos/Trabalhos/IC/Data projeto Mieloma Caio/Dados test_4_DBMM/DADOS PCMMD - Mendeley/data/segmentation\")\n",
    "REPO_DEST_PATH = Path(\"/home/ludrz/Documentos/Trabalhos/IC/Data projeto Mieloma Caio/Dados test_4_DBMM/REPOSITÓRIO_DBMM\")\n",
    "ORGANIZED_DATA_PATH = REPO_DEST_PATH / \"data\" / \"slide_and_cells\"\n",
    "CSV_OUTPUT_PATH = Path(\"/home/ludrz/Documentos/Trabalhos/IC/Data projeto Mieloma Caio/Dados test_4_DBMM/Output_CSV\")\n",
    "\n",
    "#  CONFIGURAÇÃO DO BANCO DE DADOS\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': 'root',\n",
    "    'database': 'test_4_DBMM'\n",
    "}\n",
    "\n",
    "#  MAPEAMENTO DE PACIENTES \n",
    "PATIENT_FOLDER_MAP = {\n",
    "    \"Lâmina Mieloma ALBC001\": \"ALBC001\",\n",
    "    \"Lâmina Mieloma MJ001\": \"MJ001\",\n",
    "    \"Lâmina Mieloma RP001\": \"RP001\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e187b",
   "metadata": {},
   "source": [
    "## Etapa 1: Contagem e Organização dos Arquivos de Origem\n",
    "\n",
    "Esta seção contém as funções para:\n",
    "1.  Contar todos os arquivos relevantes nas pastas de origem.\n",
    "2.  Mover os arquivos para a estrutura de diretório em `REPOSITÓRIO_DBMM`.\n",
    "3.  Verificar as contagens antes e depois para garantir que nenhum dado foi perdido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3fbc802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES DA ETAPA 1 (CONTAGEM E ORGANIZAÇÃO)\n",
    "\n",
    "def count_source_files(gdrive_path, mendeley_path, patient_map):\n",
    "    \"\"\"Conta os arquivos relevantes nas pastas de origem antes de movê-los.\"\"\"\n",
    "    logging.info(\"Contando arquivos nas fontes de dados originais...\")\n",
    "    counts = {'slides_jpg': 0, 'slides_xml': 0, 'plasma_jpg': 0, 'plasma_json': 0, 'non_plasma_jpg': 0, 'non_plasma_json': 0}\n",
    "    if not gdrive_path.exists() or not mendeley_path.exists():\n",
    "        logging.error(\"Um ou mais diretórios de origem não existem.\")\n",
    "        return None\n",
    "\n",
    "    # Itera pelos pacientes para encontrar lâminas e depois contar as células correspondentes\n",
    "    for patient_folder_name in os.listdir(gdrive_path):\n",
    "        if patient_folder_name not in patient_map: continue\n",
    "        \n",
    "        patient_gdrive_folder = gdrive_path / patient_folder_name\n",
    "        processed_slides = set()\n",
    "        for gdrive_file in os.listdir(patient_gdrive_folder):\n",
    "            gdrive_file_path = patient_gdrive_folder / gdrive_file\n",
    "            if gdrive_file_path.is_file() and gdrive_file_path.suffix.lower() in ['.jpg', '.xml']:\n",
    "                slide_base_name = gdrive_file_path.stem\n",
    "                if slide_base_name in processed_slides: continue\n",
    "                processed_slides.add(slide_base_name)\n",
    "                \n",
    "                if (patient_gdrive_folder / f\"{slide_base_name}.jpg\").exists(): counts['slides_jpg'] += 1\n",
    "                if (patient_gdrive_folder / f\"{slide_base_name}.xml\").exists(): counts['slides_xml'] += 1\n",
    "\n",
    "                # Conta células correspondentes em Mendeley\n",
    "                for cell_type in ['plasma cells', 'non-plasma cells']:\n",
    "                    cell_images_source = mendeley_path / cell_type / 'images'\n",
    "                    cell_masks_source = mendeley_path / cell_type / 'masks'\n",
    "                    if cell_images_source.exists():\n",
    "                        for cell_file in os.listdir(cell_images_source):\n",
    "                            if cell_file.lower().startswith(f\"{slide_base_name.lower()}_p_\") and cell_file.lower().endswith('.jpg'):\n",
    "                                key_jpg = 'plasma_jpg' if 'plasma' in cell_type else 'non_plasma_jpg'\n",
    "                                key_json = 'plasma_json' if 'plasma' in cell_type else 'non_plasma_json'\n",
    "                                counts[key_jpg] += 1\n",
    "                                if (cell_masks_source / f\"{Path(cell_file).stem}.json\").exists():\n",
    "                                    counts[key_json] += 1\n",
    "    return counts\n",
    "\n",
    "def organize_files_and_get_paths(gdrive_path, mendeley_path, output_repo_path, patient_map):\n",
    "    \"\"\"Move os arquivos das fontes para a nova estrutura organizada.\"\"\"\n",
    "    logging.info(f\"Movendo arquivos para a nova estrutura em {output_repo_path}...\")\n",
    "    output_repo_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for patient_folder_name, patient_id in patient_map.items():\n",
    "        patient_source_folder = gdrive_path / patient_folder_name\n",
    "        if not patient_source_folder.exists(): continue\n",
    "\n",
    "        patient_dest_folder = output_repo_path / patient_id\n",
    "        \n",
    "        processed_slides = set()\n",
    "        for gdrive_file in os.listdir(patient_source_folder):\n",
    "            gdrive_file_path = patient_source_folder / gdrive_file\n",
    "            if gdrive_file_path.is_file() and gdrive_file_path.suffix.lower() in ['.jpg', '.xml']:\n",
    "                slide_base_name = gdrive_file_path.stem\n",
    "                if slide_base_name in processed_slides: continue\n",
    "                processed_slides.add(slide_base_name)\n",
    "                \n",
    "                slide_dest_folder = patient_dest_folder / slide_base_name\n",
    "                slide_files_dest = slide_dest_folder / \"slide\"\n",
    "                slide_files_dest.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                for ext in ['.jpg', '.xml']:\n",
    "                    source_file = patient_source_folder / f\"{slide_base_name}{ext}\"\n",
    "                    if source_file.exists(): shutil.move(str(source_file), str(slide_files_dest))\n",
    "\n",
    "                for cell_type in ['plasma cells', 'non-plasma cells']:\n",
    "                    cell_dest_folder = slide_dest_folder / cell_type\n",
    "                    cell_dest_folder.mkdir(exist_ok=True)\n",
    "                    cell_images_source = mendeley_path / cell_type / 'images'\n",
    "                    cell_masks_source = mendeley_path / cell_type / 'masks'\n",
    "\n",
    "                    if cell_images_source.exists():\n",
    "                        for cell_img_filename in list(os.listdir(cell_images_source)):\n",
    "                            if cell_img_filename.lower().startswith(f\"{slide_base_name.lower()}_p_\") and cell_img_filename.lower().endswith('.jpg'):\n",
    "                                shutil.move(str(cell_images_source / cell_img_filename), str(cell_dest_folder))\n",
    "\n",
    "                    if cell_masks_source.exists():\n",
    "                         for mask_filename in list(os.listdir(cell_masks_source)):\n",
    "                            if mask_filename.lower().startswith(f\"{slide_base_name.lower()}_p_\") and mask_filename.lower().endswith('.json'):\n",
    "                                shutil.move(str(cell_masks_source / mask_filename), str(cell_dest_folder))\n",
    "    logging.info(\"Movimentação de arquivos concluída.\")\n",
    "    return output_repo_path\n",
    "\n",
    "def count_organized_files(organized_path):\n",
    "    \"\"\"Conta os arquivos na nova estrutura organizada.\"\"\"\n",
    "    logging.info(\"Contando arquivos no repositório organizado final...\")\n",
    "    counts = {'slides_jpg': 0, 'slides_xml': 0, 'plasma_jpg': 0, 'plasma_json': 0, 'non_plasma_jpg': 0, 'non_plasma_json': 0}\n",
    "    for root, _, files in os.walk(organized_path):\n",
    "        p_root = Path(root)\n",
    "        for file in files:\n",
    "            if p_root.name == 'slide' and file.endswith('.jpg'): counts['slides_jpg'] += 1\n",
    "            elif p_root.name == 'slide' and file.endswith('.xml'): counts['slides_xml'] += 1\n",
    "            elif p_root.name == 'plasma cells' and file.endswith('.jpg'): counts['plasma_jpg'] += 1\n",
    "            elif p_root.name == 'plasma cells' and file.endswith('.json'): counts['plasma_json'] += 1\n",
    "            elif p_root.name == 'non-plasma cells' and file.endswith('.jpg'): counts['non_plasma_jpg'] += 1\n",
    "            elif p_root.name == 'non-plasma cells' and file.endswith('.json'): counts['non_plasma_json'] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f76b1d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 14:45:34,334 - INFO - Contando arquivos nas fontes de dados originais...\n",
      "2025-06-30 14:45:35,234 - INFO - Contagem inicial de arquivos na origem: {'slides_jpg': 436, 'slides_xml': 433, 'plasma_jpg': 1826, 'plasma_json': 1826, 'non_plasma_jpg': 0, 'non_plasma_json': 0}\n",
      "2025-06-30 14:45:35,235 - INFO - Movendo arquivos para a nova estrutura em /home/ludrz/Documentos/Trabalhos/IC/Data projeto Mieloma Caio/Dados test_4_DBMM/REPOSITÓRIO_DBMM/data/slide_and_cells...\n",
      "2025-06-30 14:45:36,962 - INFO - Movimentação de arquivos concluída.\n",
      "2025-06-30 14:45:36,963 - INFO - Contando arquivos no repositório organizado final...\n",
      "2025-06-30 14:45:37,005 - INFO - Contagem final de arquivos no repositório: {'slides_jpg': 436, 'slides_xml': 433, 'plasma_jpg': 1087, 'plasma_json': 1087, 'non_plasma_jpg': 739, 'non_plasma_json': 739}\n",
      "2025-06-30 14:45:37,006 - ERROR - VERIFICAÇÃO ETAPA 1: FALHA! As contagens de arquivos não correspondem.\n"
     ]
    }
   ],
   "source": [
    "# EXECUÇÃO E VERIFICAÇÃO DA ETAPA 1\n",
    "\n",
    "# 1. Contar arquivos ANTES de mover\n",
    "source_counts = count_source_files(GDRIVE_SOURCE_PATH, MENDELEY_SOURCE_PATH, PATIENT_FOLDER_MAP)\n",
    "if source_counts:\n",
    "    logging.info(f\"Contagem inicial de arquivos na origem: {source_counts}\")\n",
    "\n",
    "    # 2. Executar a movimentação dos arquivos\n",
    "    organize_files_and_get_paths(GDRIVE_SOURCE_PATH, MENDELEY_SOURCE_PATH, ORGANIZED_DATA_PATH, PATIENT_FOLDER_MAP)\n",
    "    \n",
    "    # 3. Contar arquivos DEPOIS de mover\n",
    "    organized_counts = count_organized_files(ORGANIZED_DATA_PATH)\n",
    "    logging.info(f\"Contagem final de arquivos no repositório: {organized_counts}\")\n",
    "    \n",
    "    # 4. Verificação final\n",
    "    if source_counts == organized_counts:\n",
    "        logging.info(\"VERIFICAÇÃO ETAPA 1: SUCESSO! As contagens de arquivos antes e depois da movimentação correspondem.\")\n",
    "    else:\n",
    "        logging.error(\"VERIFICAÇÃO ETAPA 1: FALHA! As contagens de arquivos não correspondem.\")\n",
    "else:\n",
    "    logging.error(\"Não foi possível iniciar a Etapa 1. Verifique os caminhos de origem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ac29f",
   "metadata": {},
   "source": [
    "## Etapa 2: Extração de Metadados e Geração dos Arquivos CSV\n",
    "\n",
    "Com os arquivos agora organizados, esta seção percorre o `REPOSITÓRIO_DBMM`, extrai todos os metadados e os estrutura em 9 arquivos `.csv` na pasta `Output_CSV`. A criação destes arquivos serve como verificação  antes da importação para o banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6db93923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FUNÇÕES DA ETAPA 2 (EXTRAÇÃO E GERAÇÃO DE CSVs)\n",
    "\n",
    "\n",
    "# --- FUNÇÕES DE PARSING\n",
    "def get_image_dimensions(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img: width, height = img.size; return width, height, width * height\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Não foi possível ler dimensões de {image_path}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def parse_xml_objects(xml_file_path, slide_id_for_fk, counters):\n",
    "    objects = []\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path); root = tree.getroot();\n",
    "        for i, obj_node in enumerate(root.findall('.//object')):\n",
    "            counters['slide_object_id'] += 1; bndbox_node = obj_node.find('bndbox')\n",
    "            objects.append({\n",
    "                'slide_object_id': counters['slide_object_id'], 'slide_id': slide_id_for_fk,\n",
    "                'object_identifier_str': f\"{xml_file_path.stem}_obj_{i+1}\",\n",
    "                'object_name': (n.text if (n := obj_node.find('name')) is not None else None),\n",
    "                'xmin': (int(n.text) if bndbox_node is not None and (n := bndbox_node.find('xmin')) is not None else None),\n",
    "                'ymin': (int(n.text) if bndbox_node is not None and (n := bndbox_node.find('ymin')) is not None else None),\n",
    "                'xmax': (int(n.text) if bndbox_node is not None and (n := bndbox_node.find('xmax')) is not None else None),\n",
    "                'ymax': (int(n.text) if bndbox_node is not None and (n := bndbox_node.find('ymax')) is not None else None),\n",
    "                'pose': (n.text if (n := obj_node.find('pose')) is not None else None),\n",
    "                'truncated': (int(n.text) if (n := obj_node.find('truncated')) is not None and n.text is not None else None),\n",
    "                'difficult': (int(n.text) if (n := obj_node.find('difficult')) is not None and n.text is not None else None),\n",
    "            })\n",
    "    except Exception as e: logging.error(f\"Erro ao processar XML {xml_file_path}: {e}\")\n",
    "    return objects\n",
    "\n",
    "def parse_json_mask(json_file_path):\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as f: data = json.load(f)\n",
    "        if data.get('shapes') and len(data['shapes']) > 0:\n",
    "            shape = data['shapes'][0]; return shape.get('label'), shape.get('shape_type'), json.dumps(shape.get('points'))\n",
    "    except Exception as e: logging.warning(f\"Não foi possível ler JSON {json_file_path}: {e}\")\n",
    "    return None, None, None\n",
    "\n",
    "# --- FUNÇÃO DE EXTRAÇÃO\n",
    "def extract_data_and_write_csvs(organized_repo_path, csv_output_path):\n",
    "    logging.info(\"Iniciando a Etapa 2: Extração de dados e geração de CSVs...\")\n",
    "    data_storage = { 'image_repository': [], 'patient': [], 'image_metadata': [], 'slide': [], 'slide_object': [], 'slide_cell_info': [], 'cell': [], 'group_image_info': [], 'group_multiple_image': [] }\n",
    "    counters = { 'image_repository_id': 0, 'patient_id': 0, 'image_metadata_id': 0, 'slide_id': 0, 'slide_object_id': 0, 'slide_cell_info_id': 0, 'cell_id': 0, 'group_image_info_id': 0, 'group_multiple_image_id': 0 }\n",
    "    patient_identifier_to_id, slide_id_to_slide_cell_info_id = {}, {}\n",
    "    \n",
    "    counters['image_repository_id'] += 1\n",
    "    data_storage['image_repository'].append({'image_repository_id': counters['image_repository_id'], 'repository_name': 'REPOSITORIO_DBMM', 'root_path': str(REPO_DEST_PATH), 'require_accesses': 1, 'type_accesses': 'local_filesystem'})\n",
    "\n",
    "    for patient_folder in organized_repo_path.iterdir():\n",
    "        if not patient_folder.is_dir(): continue\n",
    "        patient_id_str = patient_folder.name; counters['patient_id'] += 1; patient_id = counters['patient_id']; patient_identifier_to_id[patient_id_str] = patient_id\n",
    "        patient_record = {'patient_id': patient_id, 'label': None, 'name': patient_id_str, 'number_of_slides': 0, 'diagnostic_suspicion': None, 'diagnosis': None, 'prognosis': None, 'multiple_myeloma_diagnose': 0}; data_storage['patient'].append(patient_record)\n",
    "        \n",
    "        for slide_folder in patient_folder.iterdir():\n",
    "            if not slide_folder.is_dir(): continue\n",
    "            slide_id_str = slide_folder.name; counters['slide_id'] += 1; slide_id = counters['slide_id'];\n",
    "            \n",
    "            counters['slide_cell_info_id'] += 1; slide_cell_info_id = counters['slide_cell_info_id']; slide_id_to_slide_cell_info_id[slide_id] = slide_cell_info_id\n",
    "            slide_cell_info_record = {'slide_cell_info_id': slide_cell_info_id, 'slide_id': slide_id, 'calculated_cell_count': 0, 'other_notes': None}; data_storage['slide_cell_info'].append(slide_cell_info_record)\n",
    "            \n",
    "            counters['group_image_info_id'] += 1; group_id_for_slide = counters['group_image_info_id']\n",
    "            data_storage['group_image_info'].append({\n",
    "                'group_image_info_id': group_id_for_slide,\n",
    "                'description': f'Files for slide {slide_id_str} of patient {patient_id_str}',\n",
    "                'relative_group_path': str(slide_folder.relative_to(ORGANIZED_DATA_PATH.parent)),\n",
    "                'image_repository_id': counters['image_repository_id']\n",
    "            })\n",
    "\n",
    "            slide_main_image_metadata_id = None\n",
    "            slide_content_path = slide_folder / \"slide\"\n",
    "            if slide_content_path.exists():\n",
    "                for item in slide_content_path.iterdir():\n",
    "                    if not item.is_file(): continue\n",
    "                    if item.suffix.lower() == '.jpg':\n",
    "                        counters['image_metadata_id'] += 1; metadata_id = counters['image_metadata_id']; width, height, num_pixels = get_image_dimensions(item)\n",
    "                        data_storage['image_metadata'].append({'image_metadata_id': metadata_id, 'image_name': item.name, 'image_type': 'jpg', 'image_date': datetime.fromtimestamp(item.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'), 'image_size_kb': round(item.stat().st_size / 1024), 'relative_path': str(item.relative_to(REPO_DEST_PATH)), 'image_width': width, 'image_height': height, 'number_of_pixels': num_pixels})\n",
    "                        slide_main_image_metadata_id = metadata_id\n",
    "                        counters['group_multiple_image_id'] += 1\n",
    "                        data_storage['group_multiple_image'].append({'group_multiple_image_id': counters['group_multiple_image_id'], 'group_image_info_id': group_id_for_slide, 'image_metadata_id': metadata_id})\n",
    "                    elif item.suffix.lower() == '.xml':\n",
    "                        data_storage['slide_object'].extend(parse_xml_objects(item, slide_id, counters))\n",
    "\n",
    "            data_storage['slide'].append({'slide_id': slide_id, 'slide_identifier': slide_id_str, 'myeloma_is_present': None, 'patient_id': patient_id, 'image_metadata_id': slide_main_image_metadata_id, 'xml_folder_name': None, 'slide_label_content': None})\n",
    "\n",
    "            total_cells_for_slide = 0\n",
    "            for cell_type_name in [\"non-plasma cells\", \"plasma cells\"]:\n",
    "                cell_folder_path = slide_folder / cell_type_name\n",
    "                if not cell_folder_path.is_dir(): continue\n",
    "                for file in cell_folder_path.iterdir():\n",
    "                    if file.is_file() and file.suffix.lower() == '.jpg':\n",
    "                        total_cells_for_slide += 1; counters['cell_id'] += 1; cell_id = counters['cell_id']\n",
    "                        counters['image_metadata_id'] += 1; cell_img_metadata_id = counters['image_metadata_id']; width, height, num_pixels = get_image_dimensions(file)\n",
    "                        data_storage['image_metadata'].append({'image_metadata_id': cell_img_metadata_id, 'image_name': file.name, 'image_type': 'jpg', 'image_date': datetime.fromtimestamp(file.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'), 'image_size_kb': round(file.stat().st_size / 1024), 'relative_path': str(file.relative_to(REPO_DEST_PATH)), 'image_width': width, 'image_height': height, 'number_of_pixels': num_pixels})\n",
    "                        \n",
    "                        counters['group_multiple_image_id'] += 1\n",
    "                        data_storage['group_multiple_image'].append({'group_multiple_image_id': counters['group_multiple_image_id'], 'group_image_info_id': group_id_for_slide, 'image_metadata_id': cell_img_metadata_id})\n",
    "\n",
    "                        mask_path = cell_folder_path / f\"{file.stem}.json\"\n",
    "                        mask_label, mask_shape_type, mask_points_str = parse_json_mask(mask_path) if mask_path.exists() else (None, None, None)\n",
    "                        \n",
    "                        cell_type_value = \"plasma\" if cell_type_name == \"plasma cells\" else \"non-plasma\"\n",
    "                        \n",
    "                        data_storage['cell'].append({\n",
    "                            'cell_id': cell_id, 'cell_identifier_str': file.stem, \n",
    "                            'slide_cell_info_id': slide_cell_info_id, \n",
    "                            'image_metadata_id': cell_img_metadata_id, \n",
    "                            'cell_type': cell_type_value,\n",
    "                            'mask_label': mask_label, \n",
    "                            'mask_shape_type': mask_shape_type, \n",
    "                            'mask_points_json_str': mask_points_str, \n",
    "                            'core_numbers': None\n",
    "                        })\n",
    "            \n",
    "            slide_cell_info_record['calculated_cell_count'] = total_cells_for_slide\n",
    "        patient_record['number_of_slides'] = len(list(patient_folder.glob('*')))\n",
    "\n",
    "    logging.info(f\"Escrevendo arquivos CSV em: {csv_output_path}...\")\n",
    "    csv_output_path.mkdir(parents=True, exist_ok=True)\n",
    "    for table_name, data_list in data_storage.items():\n",
    "        if not data_list: logging.warning(f\"Nenhum dado para a tabela '{table_name}', CSV não será gerado.\"); continue\n",
    "        df = pd.DataFrame(data_list); df.to_csv(csv_output_path / f\"{table_name}.csv\", index=False, encoding='utf-8')\n",
    "        logging.info(f\"✔ Arquivo '{table_name}.csv' gerado com sucesso ({len(df)} linhas).\")\n",
    "    \n",
    "    logging.info(\"Geração de CSVs concluída.\")\n",
    "    return {name: pd.DataFrame(data) for name, data in data_storage.items() if data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2831429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 14:45:37,047 - INFO - Iniciando a Etapa 2: Extração de dados e geração de CSVs...\n",
      "2025-06-30 14:45:38,408 - INFO - Escrevendo arquivos CSV em: /home/ludrz/Documentos/Trabalhos/IC/Data projeto Mieloma Caio/Dados test_4_DBMM/Output_CSV...\n",
      "2025-06-30 14:45:38,414 - INFO - ✔ Arquivo 'image_repository.csv' gerado com sucesso (1 linhas).\n",
      "2025-06-30 14:45:38,418 - INFO - ✔ Arquivo 'patient.csv' gerado com sucesso (3 linhas).\n",
      "2025-06-30 14:45:38,436 - INFO - ✔ Arquivo 'image_metadata.csv' gerado com sucesso (2262 linhas).\n",
      "2025-06-30 14:45:38,441 - INFO - ✔ Arquivo 'slide.csv' gerado com sucesso (436 linhas).\n",
      "2025-06-30 14:45:38,454 - INFO - ✔ Arquivo 'slide_object.csv' gerado com sucesso (1884 linhas).\n",
      "2025-06-30 14:45:38,457 - INFO - ✔ Arquivo 'slide_cell_info.csv' gerado com sucesso (436 linhas).\n",
      "2025-06-30 14:45:38,501 - INFO - ✔ Arquivo 'cell.csv' gerado com sucesso (1826 linhas).\n",
      "2025-06-30 14:45:38,506 - INFO - ✔ Arquivo 'group_image_info.csv' gerado com sucesso (436 linhas).\n",
      "2025-06-30 14:45:38,514 - INFO - ✔ Arquivo 'group_multiple_image.csv' gerado com sucesso (2262 linhas).\n",
      "2025-06-30 14:45:38,515 - INFO - Geração de CSVs concluída.\n",
      "2025-06-30 14:45:38,539 - INFO - --- Ponto de Verificação 2: Verificando DataFrames extraídos ---\n",
      "2025-06-30 14:45:38,541 - INFO - DataFrame 'image_repository': 1 linhas, 5 colunas.\n",
      "2025-06-30 14:45:38,541 - INFO - DataFrame 'patient': 3 linhas, 8 colunas.\n",
      "2025-06-30 14:45:38,542 - INFO - DataFrame 'image_metadata': 2262 linhas, 9 colunas.\n",
      "2025-06-30 14:45:38,542 - INFO - DataFrame 'slide': 436 linhas, 7 colunas.\n",
      "2025-06-30 14:45:38,542 - INFO - DataFrame 'slide_object': 1884 linhas, 11 colunas.\n",
      "2025-06-30 14:45:38,543 - INFO - DataFrame 'slide_cell_info': 436 linhas, 4 colunas.\n",
      "2025-06-30 14:45:38,543 - INFO - DataFrame 'cell': 1826 linhas, 9 colunas.\n",
      "2025-06-30 14:45:38,543 - INFO - DataFrame 'group_image_info': 436 linhas, 4 colunas.\n",
      "2025-06-30 14:45:38,543 - INFO - DataFrame 'group_multiple_image': 2262 linhas, 3 colunas.\n",
      "2025-06-30 14:45:38,544 - INFO - VERIFICAÇÃO ETAPA 2: SUCESSO! DataFrame 'cell' foi gerado com sucesso.\n",
      "2025-06-30 14:45:38,544 - INFO - --- Fim da Verificação 2 ---\n"
     ]
    }
   ],
   "source": [
    "# EXECUÇÃO E VERIFICAÇÃO DA ETAPA 2\n",
    "\n",
    "# Executa a função de extração e geração de CSVs\n",
    "dataframes_in_memory = extract_data_and_write_csvs(ORGANIZED_DATA_PATH, CSV_OUTPUT_PATH)\n",
    "\n",
    "# Ponto de Verificação 2: Inspecionar os DataFrames criados em memória\n",
    "if dataframes_in_memory:\n",
    "    logging.info(\"--- Ponto de Verificação 2: Verificando DataFrames extraídos ---\")\n",
    "    for name, df in dataframes_in_memory.items():\n",
    "        logging.info(f\"DataFrame '{name}': {df.shape[0]} linhas, {df.shape[1]} colunas.\")\n",
    "    \n",
    "    if 'cell' not in dataframes_in_memory or dataframes_in_memory['cell'].empty:\n",
    "         logging.error(\"VERIFICAÇÃO ETAPA 2: FALHA! Nenhum dado de célula foi extraído. O DataFrame 'cell' está vazio ou ausente.\")\n",
    "    else:\n",
    "         logging.info(\"VERIFICAÇÃO ETAPA 2: SUCESSO! DataFrame 'cell' foi gerado com sucesso.\")\n",
    "    logging.info(\"--- Fim da Verificação 2 ---\")\n",
    "else:\n",
    "    logging.error(\"Nenhum DataFrame foi gerado na Etapa 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a633b",
   "metadata": {},
   "source": [
    "## Etapa 3: Importação e Validação no Banco de Dados\n",
    "\n",
    "O script lê os `.csv` recém-criados, se conecta no banco de dados `test_4_DBMM`, limpa as tabelas  e importa os novos dados. Ao final, uma consulta de validação compara as contagens de linhas nos CSVs com as contagens nas tabelas do banco para uma verificação final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e77efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÕES DA ETAPA 3 (IMPORTAÇÃO E VALIDAÇÃO NO BD)\n",
    "\n",
    "def get_db_engine(db_config):\n",
    "    \"\"\"Cria e retorna uma engine de conexão do SQLAlchemy.\"\"\"\n",
    "    try:\n",
    "        connection_string = f\"mysql+mysqlconnector://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        with engine.connect() as connection:\n",
    "            logging.info(f\"Conexão com o banco '{db_config['database']}' bem-sucedida.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Falha ao conectar ao banco de dados: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_csvs_to_db(csv_path, engine):\n",
    "    \"\"\"Carrega todos os arquivos CSV de uma pasta para o banco de dados.\"\"\"\n",
    "    TABLE_IMPORT_ORDER = [\n",
    "        'image_repository', 'patient', 'image_metadata', 'group_image_info', 'slide',\n",
    "        'slide_object', 'slide_cell_info', 'cell', 'group_multiple_image'\n",
    "    ]\n",
    "    \n",
    "    logging.info(\"Iniciando a Etapa 3: Importação dos dados no banco...\")\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(text(\"SET FOREIGN_KEY_CHECKS = 0;\"))\n",
    "            for table_name in reversed(TABLE_IMPORT_ORDER):\n",
    "                logging.info(f\"  Limpando tabela '{table_name}'...\")\n",
    "                connection.execute(text(f\"TRUNCATE TABLE {table_name};\"))\n",
    "            connection.execute(text(\"SET FOREIGN_KEY_CHECKS = 1;\"))\n",
    "            logging.info(\"Todas as tabelas foram limpas (TRUNCATE).\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Não foi possível limpar as tabelas. Erro: {e}\")\n",
    "        return\n",
    "\n",
    "    for table_name in TABLE_IMPORT_ORDER:\n",
    "        csv_file = csv_path / f\"{table_name}.csv\"\n",
    "        if not csv_file.exists(): continue\n",
    "        \n",
    "        logging.info(f\"  Carregando arquivo '{csv_file.name}' para a tabela '{table_name}'...\")\n",
    "        df = pd.read_csv(csv_file).replace({np.nan: None})\n",
    "        \n",
    "        # Converte colunas que podem ser float com NaN para inteiros que aceitam nulos\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            # Verifica se a conversão é segura\n",
    "            if df[col].dropna().mod(1).sum() == 0:\n",
    "                df[col] = df[col].astype('Int64')\n",
    "\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='append', index=False)\n",
    "    logging.info(\"Importação de dados no banco concluída.\")\n",
    "\n",
    "def verify_db_load(engine, dataframes_to_check):\n",
    "    \"\"\"Compara a contagem de linhas dos DataFrames com as tabelas do banco.\"\"\"\n",
    "    logging.info(\"--- Ponto de Verificação 3: Validando dados no banco ---\")\n",
    "    all_match = True\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in dataframes_to_check.items():\n",
    "            df_count = len(df)\n",
    "            db_count_result = connection.execute(text(f\"SELECT COUNT(*) FROM {table_name};\")).scalar_one()\n",
    "            if df_count == db_count_result:\n",
    "                logging.info(f\"  Tabela '{table_name}': Contagem OK ({df_count} linhas).\")\n",
    "            else:\n",
    "                logging.error(f\"  Tabela '{table_name}': INCONSISTÊNCIA! DataFrame tem {df_count} linhas, BD tem {db_count_result} linhas.\")\n",
    "                all_match = False\n",
    "    \n",
    "    if all_match:\n",
    "        logging.info(\"VERIFICAÇÃO ETAPA 3: SUCESSO! Todas as contagens de registros no banco correspondem aos dados extraídos.\")\n",
    "    else:\n",
    "        logging.error(\"VERIFICAÇÃO ETAPA 3: FALHA! Inconsistências encontradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f9752ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 14:45:38,639 - INFO - Conexão com o banco 'test_4_DBMM' bem-sucedida.\n",
      "2025-06-30 14:45:38,640 - INFO - Iniciando a Etapa 3: Importação dos dados no banco...\n",
      "2025-06-30 14:45:38,641 - INFO -   Limpando tabela 'group_multiple_image'...\n",
      "2025-06-30 14:45:38,689 - INFO -   Limpando tabela 'cell'...\n",
      "2025-06-30 14:45:38,721 - INFO -   Limpando tabela 'slide_cell_info'...\n",
      "2025-06-30 14:45:38,743 - INFO -   Limpando tabela 'slide_object'...\n",
      "2025-06-30 14:45:38,770 - INFO -   Limpando tabela 'slide'...\n",
      "2025-06-30 14:45:38,796 - INFO -   Limpando tabela 'group_image_info'...\n",
      "2025-06-30 14:45:38,816 - INFO -   Limpando tabela 'image_metadata'...\n",
      "2025-06-30 14:45:38,844 - INFO -   Limpando tabela 'patient'...\n",
      "2025-06-30 14:45:38,866 - INFO -   Limpando tabela 'image_repository'...\n",
      "2025-06-30 14:45:38,887 - INFO - Todas as tabelas foram limpas (TRUNCATE).\n",
      "2025-06-30 14:45:38,888 - INFO -   Carregando arquivo 'image_repository.csv' para a tabela 'image_repository'...\n",
      "2025-06-30 14:45:38,914 - INFO -   Carregando arquivo 'patient.csv' para a tabela 'patient'...\n",
      "2025-06-30 14:45:38,930 - INFO -   Carregando arquivo 'image_metadata.csv' para a tabela 'image_metadata'...\n",
      "2025-06-30 14:45:39,093 - INFO -   Carregando arquivo 'group_image_info.csv' para a tabela 'group_image_info'...\n",
      "2025-06-30 14:45:39,117 - INFO -   Carregando arquivo 'slide.csv' para a tabela 'slide'...\n",
      "2025-06-30 14:45:39,140 - INFO -   Carregando arquivo 'slide_object.csv' para a tabela 'slide_object'...\n",
      "2025-06-30 14:45:39,227 - INFO -   Carregando arquivo 'slide_cell_info.csv' para a tabela 'slide_cell_info'...\n",
      "2025-06-30 14:45:39,248 - INFO -   Carregando arquivo 'cell.csv' para a tabela 'cell'...\n",
      "2025-06-30 14:45:39,502 - INFO -   Carregando arquivo 'group_multiple_image.csv' para a tabela 'group_multiple_image'...\n",
      "2025-06-30 14:45:39,563 - INFO - Importação de dados no banco concluída.\n",
      "2025-06-30 14:45:39,564 - INFO - --- Ponto de Verificação 3: Validando dados no banco ---\n",
      "2025-06-30 14:45:39,565 - INFO -   Tabela 'image_repository': Contagem OK (1 linhas).\n",
      "2025-06-30 14:45:39,566 - INFO -   Tabela 'patient': Contagem OK (3 linhas).\n",
      "2025-06-30 14:45:39,567 - INFO -   Tabela 'image_metadata': Contagem OK (2262 linhas).\n",
      "2025-06-30 14:45:39,568 - INFO -   Tabela 'slide': Contagem OK (436 linhas).\n",
      "2025-06-30 14:45:39,569 - INFO -   Tabela 'slide_object': Contagem OK (1884 linhas).\n",
      "2025-06-30 14:45:39,570 - INFO -   Tabela 'slide_cell_info': Contagem OK (436 linhas).\n",
      "2025-06-30 14:45:39,571 - INFO -   Tabela 'cell': Contagem OK (1826 linhas).\n",
      "2025-06-30 14:45:39,571 - INFO -   Tabela 'group_image_info': Contagem OK (436 linhas).\n",
      "2025-06-30 14:45:39,572 - INFO -   Tabela 'group_multiple_image': Contagem OK (2262 linhas).\n",
      "2025-06-30 14:45:39,573 - INFO - VERIFICAÇÃO ETAPA 3: SUCESSO! Todas as contagens de registros no banco correspondem aos dados extraídos.\n"
     ]
    }
   ],
   "source": [
    "# EXECUÇÃO E VERIFICAÇÃO DA ETAPA 3\n",
    "\n",
    "# A variável 'dataframes_in_memory' foi preenchida\n",
    "if dataframes_in_memory:\n",
    "    db_engine = get_db_engine(DB_CONFIG)\n",
    "    if db_engine:\n",
    "        load_csvs_to_db(CSV_OUTPUT_PATH, db_engine)\n",
    "        \n",
    "        # A verificação usa os dataframes que geraram os CSVs para comparar\n",
    "        verify_db_load(db_engine, dataframes_in_memory)\n",
    "        \n",
    "        db_engine.dispose()\n",
    "else:\n",
    "    logging.warning(\"Etapa 3 pulada pois não há DataFrames em memória para verificar.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
